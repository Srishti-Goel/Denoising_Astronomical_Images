# -*- coding: utf-8 -*-
"""denoise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kV9k7y82AKrcD6-bnohn0VtNCm22QS2Q
"""

"""## Denoising Sova"""

import tensorflow as tf
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Input
import numpy as np

#Srishti  

#-----EXPERIMENT DETAILS-----
activation_type = 'relu'
n_filters_in_input = 1
batch_size=4
optimizer = 'adam'
loss_func = tf.keras.losses.MeanSquaredError(reduction="auto", name="mean_squared_error")
metric = ['accuracy']


def conv_block(input=None, n_filters=32, dropout_prob=0, max_pooling=True):
  
  print("Incoming", type(input))
  conv = Conv2D(filters=n_filters, 
                kernel_size=3, 
                activation=activation_type, 
                padding='same')(input)
  print("Going once", type(conv))
  conv = Conv2D(filters=n_filters, 
                kernel_size=3, 
                activation=activation_type, 
                padding='same')(conv)

  if dropout_prob > 0:
    conv = tf.keras.layers.Dropout(dropout_prob)(conv)

  print("Going twice", type(conv))
    
  if max_pooling:
    next_layer = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid')(conv)
  else:
    next_layer = conv
  
  skip_connection = conv

  return next_layer, skip_connection

def upsampling_block(prev_layer, skip_layer, n_filters=32):
  up = tf.keras.layers.Conv2DTranspose(
      n_filters,
      (3,3), #kernel size
      (2,2), #strides
      'same' #padding
  )(prev_layer)

  merge = concatenate([up, skip_layer], axis=3)
  conv = Conv2D(n_filters, kernel_size=3, activation=activation_type, padding='same')(merge)
  conv = Conv2D(n_filters, kernel_size=3, activation=activation_type, padding='same')(conv)

  return conv

def unet_model(input_size=( 256,256,3), n_filters=32):
  print(type(Input((1,2))))
  inputs = Input(input_size)

  cblock1 = conv_block(inputs, n_filters)
  cblock2 = conv_block(cblock1[0], 2 * n_filters)
  cblock3 = conv_block(cblock2[0], 4 * n_filters)
  cblock4 = conv_block(cblock3[0], 8 * n_filters)
  cblock5 = conv_block(cblock4[0], 16 * n_filters, max_pooling=False)
  ublock6 = upsampling_block(cblock5[0], cblock4[1], 8 * n_filters)
  ublock7 = upsampling_block(ublock6, cblock3[1], 4 * n_filters)
  ublock8 = upsampling_block(ublock7, cblock2[1], 2 * n_filters)
  ublock9 = upsampling_block(ublock8, cblock1[1], n_filters)
  conv9 = Conv2D(n_filters,
                 3,
                 activation=activation_type,
                 padding='same',
                 kernel_initializer='he_normal')(ublock9)
  conv10 = Conv2D(n_filters_in_input, 1 , padding='same')(conv9)

  model = tf.keras.Model(inputs=inputs, outputs=conv10)

  return model

model = unet_model()
model.summary()

import pandas as pd
import io

dftrain = pd.read_csv("training_data.txt",header=None)
print(dftrain)

dfeval = pd.read_csv('eval_data.txt',header=None)

print(dfeval)

dftest = pd.read_csv('test_data.txt',header=None)

print(dftest)

import matplotlib.pyplot as plt
from astropy.visualization import astropy_mpl_style
plt.style.use(astropy_mpl_style)

from astropy.utils.data import get_pkg_data_filename
from astropy.io import fits

image_file = get_pkg_data_filename('skv1278992662741.fits')

fits.info(image_file)

image_data = fits.getdata(image_file, ext=0)

plt.figure()
plt.imshow(image_data,cmap='gray')
plt.colorbar()

from astroquery.mast import Catalogs
from astroquery.vizier import Vizier
from astroquery.mast import Observations
import urllib.request
import os

filter_wide = ["F555W","F606W"]
for f in filter_wide:
    if not os.path.isdir("data/" + f + "/"):
        os.makedirs("data/" + f + "/")
    obsTable = Observations.query_criteria(calib_level= 3, dataproduct_type = 'image',
                                               obs_collection = ["HLA"],
                                        instrument_name = ["WFC3/UVIS"], filters = [f])

    for url in obsTable['dataURL']:
        if isinstance(url, type(obsTable[7]['dataURL'])):
            if url[-5:] == str(".fits"):
              if url[-1]:
                file = "data/" + f + "/" + str(url[50:])
                urllib.request.urlretrieve(url,file)

#Srishti

#---------TRAINING PARAMETERS------------
n_epochs = 2


#----------TRANSFORMING IMAGES------------
def noise_adder(image_data, ratio, exposure_time, patch_noise=False, xx=None, yy=None, ps=None, DN=True, RON=True, PSN=True, ron=1, dk=1, return_noisy=False):
  #image_data = Image(name)
  if patch_noise:
    image_data = image_data[xx:xx+ps, yy:yy+ps]
  width, height = image_data.shape
  img = image_data*exposure_time

  dark_noise = np.random.normal(0, np.sqrt(dk * exposure_time/(3600*ratio)), (width, height))
  read_out_noise = np.random.normal(0, ron, (width, height))
  photon_shot_noise = np.random.poisson(np.abs(img/ratio))

  total_noise = (dark_noise if DN else 0) + (read_out_noise if RON else 0) + (photon_shot_noise if PSN else 0)
  total_noise = total_noise / (exposure_time / ratio)
  total_noise = np.where(image_data == 0.00, 0.00, total_noise)
  
  if return_noisy:
    return total_noise

noisy_image = noise_adder(image_data, 2, 100, return_noisy=True)

plt.figure()
plt.imshow(noisy_image,cmap='gray')
plt.colorbar()

model.compile(optimizer=optimizer, loss=loss_func, metrics=metric )
model_history = model.fit(train_dataset, n_epochs)

